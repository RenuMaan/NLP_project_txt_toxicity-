# NLP_project_txt_toxicity-
the python code classifies a text as toxic or non-toxic and also give a bar plot showing the percentage of words that fall in the pre defined catagories, toxic, severe toxic, obscene, threat, insult and identity hate.
models folder contains pickled model and vectorizer
templates folder include html templates
app.py generates plots that are saved in static folder and are rendered on the html page
